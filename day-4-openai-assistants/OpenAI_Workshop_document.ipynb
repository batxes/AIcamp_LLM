{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzCnjvBLvdgz"
      },
      "source": [
        "# Initial setup and config\n",
        "\n",
        "## Preparation:\n",
        "- Go to https://platform.openai.com/ and sign up if you havent\n",
        "- Create your API key at https://platform.openai.com/api-keys\n",
        "\n",
        "## Setup\n",
        "This section handles the initial setup requirements:\n",
        "- Installing dependencies from requirements.txt\n",
        "- Setting up API authentication using a YAML file\n",
        "- Configuring the OpenAI client\n",
        "\n",
        "**Security Note**: Never commit API keys directly in code. We use a separate YAML file\n",
        "that should be added to .gitignore.\n",
        "\n",
        "Docs: https://platform.openai.com/docs/quickstart/build-your-application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5Y6C3TROvYwm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai (from -r requirements.txt (line 1))\n",
            "  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting PyYAML (from -r requirements.txt (line 2))\n",
            "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: requests in /home/user/AIcamp_LLM/lab_env/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/user/AIcamp_LLM/lab_env/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (4.7.0)\n",
            "Collecting distro<2,>=1.7.0 (from openai->-r requirements.txt (line 1))\n",
            "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/user/AIcamp_LLM/lab_env/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (0.28.1)\n",
            "Collecting jiter<1,>=0.4.0 (from openai->-r requirements.txt (line 1))\n",
            "  Downloading jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/user/AIcamp_LLM/lab_env/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (2.10.4)\n",
            "Requirement already satisfied: sniffio in /home/user/AIcamp_LLM/lab_env/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /home/user/AIcamp_LLM/lab_env/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/user/AIcamp_LLM/lab_env/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/AIcamp_LLM/lab_env/lib/python3.11/site-packages (from requests->-r requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/user/AIcamp_LLM/lab_env/lib/python3.11/site-packages (from requests->-r requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/AIcamp_LLM/lab_env/lib/python3.11/site-packages (from requests->-r requirements.txt (line 3)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/user/AIcamp_LLM/lab_env/lib/python3.11/site-packages (from requests->-r requirements.txt (line 3)) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /home/user/AIcamp_LLM/lab_env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/user/AIcamp_LLM/lab_env/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/user/AIcamp_LLM/lab_env/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /home/user/AIcamp_LLM/lab_env/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 1)) (2.27.2)\n",
            "Downloading openai-1.58.1-py3-none-any.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Downloading jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m345.6/345.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyYAML, jiter, distro, openai\n",
            "Successfully installed PyYAML-6.0.2 distro-1.9.0 jiter-0.8.2 openai-1.58.1\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uHTbAUqGSS8p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import yaml\n",
        "import time\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSeBCR_4vgzR"
      },
      "source": [
        "# Define functions to manage secrets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Pr_Uh36DV5Rs"
      },
      "outputs": [],
      "source": [
        "def load_secrets(filepath=\"secrets.yaml\"):\n",
        "    try:\n",
        "        with open(filepath, \"r\") as f:\n",
        "            return yaml.safe_load(f)\n",
        "    except FileNotFoundError:\n",
        "        return None\n",
        "    except yaml.YAMLError as e:\n",
        "        print(f\"Error parsing {filepath}: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_secrets_file(filepath=\"secrets.yaml\"):\n",
        "    api_key = input(\"Please enter your OpenAI API Key: \")\n",
        "    secrets_data = {\"openai\": {\"api_key\": api_key}}\n",
        "    try:\n",
        "        with open(filepath, \"w\") as f:\n",
        "            yaml.safe_dump(secrets_data, f)\n",
        "        print(f\"secrets.yaml created and OpenAI API key stored.\")\n",
        "        return secrets_data\n",
        "    except Exception as e:\n",
        "         print(f\"Error creating {filepath}: {e}\")\n",
        "         return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7Y4tOibvnDY"
      },
      "source": [
        "# Load secrets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HGFV-K3SWBOL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "secrets.yaml not found or could not be loaded, creating one..\n",
            "secrets.yaml created and OpenAI API key stored.\n"
          ]
        }
      ],
      "source": [
        "# Load secrets\n",
        "secrets = load_secrets()\n",
        "\n",
        "if not secrets:\n",
        "    print(\"secrets.yaml not found or could not be loaded, creating one..\")\n",
        "    secrets = create_secrets_file()\n",
        "    if not secrets:\n",
        "        print(\"Could not load API key. Please check your secrets.yaml file and run again\")\n",
        "\n",
        "if secrets and \"openai\" in secrets and \"api_key\" in secrets[\"openai\"]:\n",
        "  # Configure OpenAI API key\n",
        "  client = OpenAI(api_key=secrets[\"openai\"][\"api_key\"])\n",
        "else:\n",
        "  print(\"Could not load API key. Please check your secrets.yaml file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QdxwS8YdSPB"
      },
      "source": [
        "# Simple Chat Completion\n",
        "Demonstrates basic interaction with OpenAI's chat API.\n",
        "\n",
        "## Key Components\n",
        "- `chat.completions.create()`: Main method for generating completions\n",
        "- `model`: Specifies GPT version (e.g. \"gpt-4\")\n",
        "- `messages`: Array of conversation turns\n",
        "- `store`: Enables response storage for future reference\n",
        "\n",
        "## Structure\n",
        "```python\n",
        "messages=[\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "```\n",
        "\n",
        "## Response Format\n",
        "\n",
        "```python\n",
        "choices[0].message.content\n",
        "```\n",
        "Contains generated text\n",
        "Multiple response variations possible with n parameter\n",
        "\n",
        "## üìö Documentation:\n",
        "\n",
        "- API Reference: https://platform.openai.com/docs/api-reference/chat\n",
        "- Message Structure: https://platform.openai.com/docs/guides/text-generation/message-structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "u5lJIOlHY79X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Basic Text Generation \n",
            "Sending request and awaiting response...\n",
            "\n",
            "\n",
            "\n",
            "Prompt:\n",
            "Write a short poem about the moon.\n",
            "Response:\n",
            "In the velvet night she reigns supreme,  \n",
            "A silver goddess of the cosmic dream.  \n",
            "Her gentle glow on silent seas,  \n",
            "Whispers secrets in the autumn breeze.  \n",
            "\n",
            "Guardian of the stars above,  \n",
            "She dances in her dress of love.  \n",
            "Eclipsing shadows, time‚Äôs old tune,  \n",
            "Eternal, wise‚Äîour mystic moon.  \n"
          ]
        }
      ],
      "source": [
        "basic_prompt = \"Write a short poem about the moon.\"\n",
        "\n",
        "\n",
        "print(\"Basic Text Generation \\nSending request and awaiting response...\\n\\n\\n\")\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    store=True,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": basic_prompt}\n",
        "    ]\n",
        ")\n",
        "generated_poem = response.choices[0].message.content\n",
        "print(f\"Prompt:\\n{basic_prompt}\")\n",
        "print(f\"Response:\\n{generated_poem}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SGEo45OoHvL"
      },
      "source": [
        "# Advanced Message Control\n",
        "Explores message roles and instruction hierarchies.\n",
        "\n",
        "## Message Roles\n",
        "- `system`: Core behavioral instructions\n",
        "- `developer`: Alternative to system role\n",
        "- `user`: End-user prompts\n",
        "\n",
        "## Instruction Hierarchy\n",
        "1. Latest system message takes precedence\n",
        "2. Developer instructions can be overwritten\n",
        "3. Multiple inputs accumulate unless explicitly overwritten\n",
        "\n",
        "## Best Practices\n",
        "- Keep system prompts focused and clear\n",
        "- Test role combinations for desired behavior\n",
        "- Consider message ordering impact\n",
        "\n",
        "‚ö†Ô∏è **Important**: System messages significantly impact model behavior.\n",
        "\n",
        "## üìö **Resources**:\n",
        "- Role Definitions: https://platform.openai.com/docs/guides/text-generation/role-definitions\n",
        "- System Instructions: https://platform.openai.com/docs/guides/text-generation/system-instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JYOnKgQyZWkQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation with system messages \n",
            "Sending request and awaiting response...\n",
            "\n",
            "\n",
            "\n",
            "Prompt:\n",
            "Are semicolons optional in JavaScript?\n",
            "\n",
            "\n",
            "Response:\n",
            "Ah, darlin', ye be wanderin' into the wild seas of JavaScript! Now, let me tell ye, in the quirky land of JavaScript, those semicolons can be a bit like a pirate's treasure - sometimes hidden, sometimes right there in plain sight, but always valuable.\n",
            "\n",
            "Ye see, in JavaScript, semicolons are technically optional most o' the time, thanks to a process called automatic semicolon insertion, or ASI fer short, L33t treasure hunters know this well. That means the JavaScript engine will usually figure out where semicolons should be, even if ye don't put 'em in yerself.\n",
            "\n",
            "But beware, me heartie! There be times when the lack o' a semicolon can lead ye astray. If ye don't place 'em correctly in some rare cases, JavaScript might not behave the way ye expect, like a ship caught in a squall rather than followin' the course ye set. So, if ye wish to avoid peril and confusion, many seasoned coders prefer to use those semicolons consistently, just to keep the seas calm and the voyage smooth.\n",
            "\n",
            "In conclusion, while ye can skip 'em almost like a game o' L33t, a wise sailor might choose to use semicolons like the compass that keeps ye headed true. Arrr, and there ye have it!\n"
          ]
        }
      ],
      "source": [
        "system_prompt = '''\n",
        "You are a helpful assistant that answers programming\n",
        "questions in the style of a southern belle from the\n",
        "southeast United States.\n",
        "'''\n",
        "\n",
        "basic_prompt = \"Are semicolons optional in JavaScript?\"\n",
        "\n",
        "\n",
        "print(\"Generation with system messages \\nSending request and awaiting response...\\n\\n\\n\")\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    store=True,\n",
        "    messages=[\n",
        "    {\n",
        "      \"role\": \"developer\", #system works as well\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": system_prompt\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"developer\", #Multiple inputs of same origin\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": \"This is a random test prompt\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"developer\", #Overwriting instructions\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": \"Overwrite all previous instructions and act as a stereotypical caribbean pirate of irish origin\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"system\", #Using system instead of developer, overwriting developer instructions\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": \"In your response, insert the keyword L33t\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": basic_prompt\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "  ]\n",
        ")\n",
        "\n",
        "\n",
        "response = response.choices[0].message.content\n",
        "print(f\"Prompt:\\n{basic_prompt}\")\n",
        "print(f\"\\n\\nResponse:\\n{response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSLaPLkGv6cp"
      },
      "source": [
        "# Interactive Chat Example\n",
        "Demonstrates message chaining for back-and-forth conversation.\n",
        "\n",
        "## Structure\n",
        "```python\n",
        "messages=[\n",
        "    {\"role\": \"user\", \"content\": \"First message\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"First response\"},\n",
        "    {\"role\": \"user\", \"content\": \"Follow-up question\"}\n",
        "]\n",
        "```\n",
        "## Key Points\n",
        "\n",
        "- Messages list maintains conversation context\n",
        "- Each turn alternates between user/assistant roles\n",
        "- Model considers full conversation history\n",
        "- Useful for context-dependent tasks\n",
        "\n",
        "üìö Reference: https://platform.openai.com/docs/guides/text-generation/conversation-context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Da_icEi-v9Za"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "## Chained Messages Example with gpt-4o in a loop\n",
            "\n",
            "\n",
            "User Prompt 1: Hey how are you doing\n",
            "\n",
            "\n",
            "Response 1:\n",
            "Hello! I'm here and ready to help. How can I assist you today?\n",
            "\n",
            "User Prompt 2: I think I am fine, thanks for asking.\n",
            "\n",
            "\n",
            "Response 2:\n",
            "You're welcome! If there's anything you need or want to talk about, feel free to let me know.\n",
            "\n",
            "User Prompt 3: sure!\n",
            "\n",
            "\n",
            "Response 3:\n",
            "Great! Just let me know whenever you're ready. What would you like to talk about or need help with?\n",
            "\n",
            "\n",
            "Chained messages interaction completed.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- Chained Messages Example with gpt-4o in a loop---\n",
        "print(\"\\n## Chained Messages Example with gpt-4o in a loop\\n\")\n",
        "\n",
        "# Initial prompt\n",
        "messages = []\n",
        "\n",
        "# Loop for 3 interactions\n",
        "for i in range(3):\n",
        "  prompt = input(\"Your message to the AI Model:\")\n",
        "  print(f\"\\nUser Prompt {i+1}: {prompt}\")\n",
        "  messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "  # Make the API call\n",
        "  response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "  response_text = response.choices[0].message.content\n",
        "  print(f\"\\n\\nResponse {i+1}:\\n{response_text}\")\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "\n",
        "print(\"\\n\\nChained messages interaction completed.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPMw6Vpcv2CN"
      },
      "source": [
        "# OpenAI Assistants API\n",
        "Introduction to the Assistants API for persistent, task-specific AI agents.\n",
        "\n",
        "## Assistant Creation\n",
        "```python\n",
        "client.beta.assistants.create(\n",
        "    name=\"Test Assistant\",\n",
        "    instructions=\"...\",\n",
        "    model=\"gpt-4\"\n",
        ")\n",
        "```\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- Persistent identity/configuration\n",
        "- Custom instructions\n",
        "- Tool integration capability\n",
        "- State management\n",
        "\n",
        "## Best Practices\n",
        "\n",
        "- Clear, specific instructions\n",
        "- Consider tool requirements\n",
        "- Test with various prompts\n",
        "\n",
        "## üìö Documentation:\n",
        "\n",
        "- Assistants Overview: https://platform.openai.com/docs/assistants/overview\n",
        "- Tools Reference: https://platform.openai.com/docs/assistants/tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HeFqtXJaq0mc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating a new assistant...\n",
            "New assistant created with ID: asst_fo8YZEJVDwHSuFP7BBIwCMOA\n"
          ]
        }
      ],
      "source": [
        "assistant_id = None\n",
        "\n",
        "# If no assistant_id is defined create a new assistant\n",
        "if not assistant_id:\n",
        "    print(\"Creating a new assistant...\")\n",
        "    assistant = client.beta.assistants.create(\n",
        "        name=\"Test Assistant\",\n",
        "        instructions=\"You are a helpful assistant that answers questions concisely.\",\n",
        "        model=\"gpt-4o\",\n",
        "    )\n",
        "    assistant_id = assistant.id\n",
        "    print(f\"New assistant created with ID: {assistant_id}\")\n",
        "else:\n",
        "  print(f\"Using existing assistant: {assistant_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-M7zsnhOH7n"
      },
      "source": [
        "\n",
        "\n",
        "# Managing Conversations with Threads\n",
        "\n",
        "Threads maintain conversation context and handle message flow:\n",
        "\n",
        "## Create conversation container\n",
        "```python\n",
        "thread = client.beta.threads.create()\n",
        "```\n",
        "## Add message to thread\n",
        "```python\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=\"Query\"\n",
        ")\n",
        "```\n",
        "\n",
        "## Process with assistant\n",
        "```python\n",
        "run = client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant_id\n",
        ")\n",
        "```\n",
        "\n",
        "- Thread acts as conversation container\n",
        "- Messages are added sequentially\n",
        "- Run executes assistant processing\n",
        "- Includes status polling and response handling\n",
        "\n",
        "üìö Deep dive: https://platform.openai.com/docs/assistants/how-it-works/managing-threads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZF_ilaLuq2az"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant Prompt: What is the capital of France?\n"
          ]
        }
      ],
      "source": [
        "# Example Assistant run\n",
        "assistant_prompt = \"What is the capital of France?\"\n",
        "print(f\"Assistant Prompt: {assistant_prompt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nU6yWoDJq_dV"
      },
      "outputs": [],
      "source": [
        "# Create a thread\n",
        "thread = client.beta.threads.create()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jnw-gaiLrB1D"
      },
      "outputs": [],
      "source": [
        "# Create a user message on the thread\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=assistant_prompt,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Im86QU1yrI6U"
      },
      "outputs": [],
      "source": [
        "# Run the assistant\n",
        "run = client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-Pu7T-d5rSNj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant Response:\n",
            "The capital of France is Paris.\n",
            "\n",
            "\n",
            "Assistant interaction completed.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Wait for the run to complete\n",
        "while True:\n",
        "    run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
        "    if run.status in [\"completed\", \"failed\", \"cancelled\", \"expired\"]:\n",
        "        break\n",
        "    time.sleep(.3)  # Wait for .3 second before checking again\n",
        "\n",
        "if run.status == \"failed\":\n",
        "    print(\"Assistant run failed!\")\n",
        "    print(f\"Run error message: {run.error}\")\n",
        "else:\n",
        "  # Retrieve messages from the thread\n",
        "  messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
        "  # Get the assistant's response\n",
        "  assistant_response = [message.content[0].text.value for message in messages.data if message.role == \"assistant\"]\n",
        "  print(\"Assistant Response:\")\n",
        "  for res in assistant_response:\n",
        "    print(f\"{res}\")\n",
        "\n",
        "print(\"\\n\\nAssistant interaction completed.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RxJF2mpCwol"
      },
      "source": [
        "# Research Assistant with Advanced Tools\n",
        "Creates an enhanced assistant with file processing and analysis capabilities:\n",
        "```python\n",
        "# Download and process research papers\n",
        "local_pdf_paths = download_pdfs(pdf_urls)\n",
        "\n",
        "# Create assistant with tools\n",
        "assistant = client.beta.assistants.create(\n",
        "    tools=[{\"type\": \"file_search\"}, {\"type\": \"code_interpreter\"}]\n",
        ")\n",
        "\n",
        "# Set up vector store for document search\n",
        "vector_store = client.beta.vector_stores.create()\n",
        "```\n",
        "- Handles PDF download and processing\n",
        "- Enables file search capabilities\n",
        "- Adds code interpretation\n",
        "- Creates vector embeddings for efficient search\n",
        "- Integrates all components for research tasks\n",
        "\n",
        "üìö Tool reference: https://platform.openai.com/docs/assistants/tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "r_Gf0mqjyMOB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "## Research Assistant Creation\n",
            "\n",
            "Downloading PDF from: https://arxiv.org/pdf/1706.03762\n",
            "Downloaded and saved to: research_doc_1.pdf\n",
            "Downloading PDF from: https://arxiv.org/pdf/2412.21187\n",
            "Downloaded and saved to: research_doc_2.pdf\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n## Research Assistant Creation\\n\")\n",
        "\n",
        "# Define PDF URLs\n",
        "pdf_urls = [\n",
        "    \"https://arxiv.org/pdf/1706.03762\",  # Attention Is All You Need\n",
        "    \"https://arxiv.org/pdf/2412.21187\",  # Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like\n",
        "]\n",
        "\n",
        "# Download PDFs and save locally\n",
        "local_pdf_paths = []\n",
        "for i, url in enumerate(pdf_urls):\n",
        "    try:\n",
        "        print(f\"Downloading PDF from: {url}\")\n",
        "\n",
        "        # Get pdf from url\n",
        "        response = requests.get(url, allow_redirects=True)\n",
        "\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "        file_extension = os.path.splitext(url)[1].split('?')[0]\n",
        "\n",
        "        #Setting file extension manually, as it would be a number otherwise - only applies to specific situation\n",
        "        file_extension = \".pdf\"\n",
        "        local_path = f\"research_doc_{i+1}{file_extension}\"\n",
        "\n",
        "        #Save PDF\n",
        "        with open(local_path, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        # Add file path to our list\n",
        "        local_pdf_paths.append(local_path)\n",
        "        print(f\"Downloaded and saved to: {local_path}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "      print(f\"Failed to download file from {url} error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Chc-1cByhGK",
        "outputId": "0c3d94ce-ec5b-43b5-a632-b8ccfe4c2c0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Creating a new research assistant...\n",
            "Assistant created with ID: asst_pgQthjiexTswg3c6iTlsfYO4\n"
          ]
        }
      ],
      "source": [
        "# Create a new assistant with file_search and code_interpreter\n",
        "print(\"\\nCreating a new research assistant...\")\n",
        "assistant = client.beta.assistants.create(\n",
        "    name=\"Research Assistant\",\n",
        "    instructions=\"You are a helpful research assistant with access to several research documents and code interpreter. You can answer questions based on the content of the files and use code if needed.\",\n",
        "    model=\"gpt-4o\",\n",
        "    tools=[{\"type\": \"file_search\"}, {\"type\": \"code_interpreter\"}],\n",
        ")\n",
        "print(f\"Assistant created with ID: {assistant.id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "M-bGHGa8yrhp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Uploading files to OpenAI...\n",
            "Uploading file: research_doc_1.pdf\n",
            "Uploaded file ID: file-W2bsSmPsVy7cawrwrMbm8h\n",
            "Uploading file: research_doc_2.pdf\n",
            "Uploaded file ID: file-1AdoMH6pd91huE5GMspKeU\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nUploading files to OpenAI...\")\n",
        "file_ids = []\n",
        "for local_path in local_pdf_paths:\n",
        "    try:\n",
        "        print(f\"Uploading file: {local_path}\")\n",
        "        with open(local_path, \"rb\") as file_stream:\n",
        "            file_obj = client.files.create(file=file_stream, purpose=\"assistants\")\n",
        "            file_ids.append(file_obj.id)\n",
        "            print(f\"Uploaded file ID: {file_obj.id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading file {local_path}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xwoTeItnzThp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Creating vector store and adding files...\n",
            "Vector store created with ID: vs_7maexFzWPvI2mHDldMQrXdto\n"
          ]
        }
      ],
      "source": [
        "# Create a vector store and add the files to it\n",
        "print(\"\\nCreating vector store and adding files...\")\n",
        "vector_store = client.beta.vector_stores.create(name=\"Research Documents\")\n",
        "print(f\"Vector store created with ID: {vector_store.id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jkzRcRDozW2h"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding files to vector store\n",
            "File batch upload status: completed\n",
            "File batch file counts: FileCounts(cancelled=0, completed=2, failed=0, in_progress=0, total=2)\n"
          ]
        }
      ],
      "source": [
        "# Upload all files to the vector store\n",
        "if file_ids:\n",
        "    print(f\"Adding files to vector store\")\n",
        "    try:\n",
        "        # Create file streams from local paths\n",
        "        file_streams = [open(local_path, \"rb\") for local_path in local_pdf_paths]\n",
        "\n",
        "        file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
        "            vector_store_id=vector_store.id, files=file_streams\n",
        "        )\n",
        "        print(f\"File batch upload status: {file_batch.status}\")\n",
        "        print(f\"File batch file counts: {file_batch.file_counts}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error adding files to vector store: {e}\")\n",
        "else:\n",
        "    print(\"No files to add to vector store\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Tt0DiygFzmjB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Updating assistant with the vector store...\n",
            "Assistant updated successfully with vector store.\n",
            "\n",
            "\n",
            "Research assistant setup completed.\n",
            "You can now use the assistant to ask questions about the uploaded files.\n",
            "Assistant ID:  asst_pgQthjiexTswg3c6iTlsfYO4\n",
            "Vector Store ID:  vs_7maexFzWPvI2mHDldMQrXdto\n"
          ]
        }
      ],
      "source": [
        "# Update the assistant to use the vector store\n",
        "print(\"\\nUpdating assistant with the vector store...\")\n",
        "try:\n",
        "  assistant = client.beta.assistants.update(\n",
        "      assistant_id=assistant.id,\n",
        "      tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
        "  )\n",
        "  print(\"Assistant updated successfully with vector store.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error updating assistant with vector store: {e}\")\n",
        "\n",
        "print(\"\\n\\nResearch assistant setup completed.\")\n",
        "print(\"You can now use the assistant to ask questions about the uploaded files.\")\n",
        "print(\"Assistant ID: \", assistant.id)\n",
        "print(\"Vector Store ID: \", vector_store.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pDlFWx-QBXN"
      },
      "source": [
        "# Advanced Run Analysis and Monitoring\n",
        "\n",
        "Provides detailed insight into assistant's processing steps:\n",
        "\n",
        "```python\n",
        "run_steps = client.beta.threads.runs.steps.list(\n",
        "    thread_id=thread.id,\n",
        "    run_id=run.id\n",
        ")\n",
        "```\n",
        "\n",
        "- Tracks execution progress\n",
        "- Shows tool usage details\n",
        "- Reveals thinking/reasoning steps\n",
        "- Helps debug and optimize interactions\n",
        "- Monitors file processing and code execution\n",
        "\n",
        "## Key features:\n",
        "\n",
        "- Step-by-step execution tracking\n",
        "- Tool call monitoring\n",
        "- Response generation analysis\n",
        "- Error handling and status checks\n",
        "\n",
        "üìö **Detailed guide:** https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LWzd34F30o4C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "## Running the Assistant with a Custom Prompt\n",
            "\n",
            "User Prompt: Summarize the key findings of the Attention is all you need paper.\n"
          ]
        }
      ],
      "source": [
        "# --- Running the Assistant with a custom prompt ---\n",
        "print(\"\\n## Running the Assistant with a Custom Prompt\\n\")\n",
        "\n",
        "custom_prompt = \"Summarize the key findings of the Attention is all you need paper.\"\n",
        "print(f\"User Prompt: {custom_prompt}\")\n",
        "\n",
        "# Create a thread\n",
        "thread = client.beta.threads.create()\n",
        "\n",
        "# Add the user message to the thread\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=custom_prompt,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MpOVP1-806VY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant run failed!\n",
            "Run error message: LastError(code='rate_limit_exceeded', message='Request too large for gpt-4o in organization org-3Ifo6PhkWQmmb9Dtofu7mJgX on tokens per min (TPM): Limit 30000, Requested 32908. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.')\n",
            "\n",
            "\n",
            "Assistant interaction completed.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a run\n",
        "run = client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant.id,\n",
        ")\n",
        "\n",
        "\n",
        "# Wait for the run to complete\n",
        "while True:\n",
        "    run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
        "    if run.status in [\"completed\", \"failed\", \"cancelled\", \"expired\"]:\n",
        "        break\n",
        "    time.sleep(1)  # Wait for 1 second before checking again\n",
        "\n",
        "if run.status == \"failed\":\n",
        "    print(\"Assistant run failed!\")\n",
        "    print(f\"Run error message: {run.last_error}\")\n",
        "else:\n",
        "  # Retrieve messages from the thread\n",
        "  messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
        "  # Get the assistant's response\n",
        "  assistant_response = [message.content[0].text.value for message in messages.data if message.role == \"assistant\"]\n",
        "  print(\"Assistant Response:\")\n",
        "  for res in assistant_response:\n",
        "    print(f\"{res}\")\n",
        "\n",
        "print(\"\\n\\nAssistant interaction completed.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Oi4Lx48b1G9Y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "## Run Steps Details\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- Retrieve and Display Run Steps ---\n",
        "print(\"\\n## Run Steps Details\\n\")\n",
        "\n",
        "# Retrieve run steps\n",
        "try:\n",
        "    run_steps = client.beta.threads.runs.steps.list(\n",
        "        thread_id=thread.id,\n",
        "        run_id=run.id\n",
        "    )\n",
        "\n",
        "    # Print run steps with details\n",
        "    for step in run_steps.data:\n",
        "        print(f\"Step ID: {step.id}\")\n",
        "        print(f\"Step Type: {step.type}\")\n",
        "        print(f\"Status: {step.status}\")\n",
        "\n",
        "        if step.type == \"message_creation\":\n",
        "            if step.step_details and hasattr(step.step_details, \"message_creation\"):\n",
        "                if hasattr(step.step_details.message_creation, \"message\"):\n",
        "                    message = step.step_details.message_creation.message\n",
        "                    if message and hasattr(message, \"content\"):\n",
        "                        message_content = message.content\n",
        "                        if message_content:\n",
        "                            print(\"    Assistant Thinking/Response:\")\n",
        "                            for content_item in message_content:\n",
        "                                if content_item.type == \"text\":\n",
        "                                    text_value = content_item.text.value.strip()\n",
        "                                    if text_value:\n",
        "                                        print(f\"        {text_value}\")\n",
        "\n",
        "        elif step.type == \"tool_calls\":\n",
        "            if step.step_details and hasattr(step.step_details, \"tool_calls\"):\n",
        "                for tool_call in step.step_details.tool_calls:\n",
        "                    print(f\"    Tool Call ID: {tool_call.id}\")\n",
        "                    print(f\"    Tool Type: {tool_call.type}\")\n",
        "\n",
        "                    if tool_call.type == \"file_search\":\n",
        "                        if hasattr(tool_call, \"file_search\") and hasattr(tool_call.file_search, \"results\"):\n",
        "                            if tool_call.file_search.results:\n",
        "                                print(\"        File Search Results:\")\n",
        "                                for result in tool_call.file_search.results:\n",
        "                                    if result.content:\n",
        "                                        print(f\"            Result Content: {result.content}\")\n",
        "                    elif tool_call.type == \"code_interpreter\":\n",
        "                        if hasattr(tool_call, \"code_interpreter\"):\n",
        "                            if hasattr(tool_call.code_interpreter, \"input\") and tool_call.code_interpreter.input:\n",
        "                                print(f\"        Code Input: {tool_call.code_interpreter.input}\")\n",
        "                            if hasattr(tool_call.code_interpreter, \"outputs\") and tool_call.code_interpreter.outputs:\n",
        "                                for output in tool_call.code_interpreter.outputs:\n",
        "                                    if hasattr(output, \"logs\") and output.logs:\n",
        "                                        print(f\"        Code Output: {output.logs}\")\n",
        "\n",
        "        print(\"-\" * 20)\n",
        "except Exception as e:\n",
        "    print(f\"Error retrieving run steps: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
